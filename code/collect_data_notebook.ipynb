{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWX6-CO5mnb6",
        "outputId": "b179095b-d5e1-403b-cbd3-e023a046dab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.7.2\n",
            "    Uninstalling openai-1.7.2:\n",
            "      Successfully uninstalled openai-1.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "Collecting autogen\n",
            "  Downloading autogen-1.0.16-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from autogen) (6.0.1)\n",
            "Collecting autopep8 (from autogen)\n",
            "  Downloading autopep8-2.0.4-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt (from autogen)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogen) (67.7.2)\n",
            "Collecting twine (from autogen)\n",
            "  Downloading twine-4.0.2-py3-none-any.whl (36 kB)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8->autogen)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8->autogen) (2.0.1)\n",
            "Collecting pkginfo>=1.8.1 (from twine->autogen)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine->autogen)\n",
            "  Downloading readme_renderer-42.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from twine->autogen) (2.31.0)\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0 (from twine->autogen)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from twine->autogen) (2.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.10/dist-packages (from twine->autogen) (6.11.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/lib/python3/dist-packages (from twine->autogen) (23.5.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine->autogen)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from twine->autogen) (13.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.6->twine->autogen) (3.17.0)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->autogen)\n",
            "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->autogen) (0.18.1)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->autogen) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->autogen) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->autogen) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->autogen) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->twine->autogen) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine->autogen) (0.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=929f7fdeaa13c66a8cf687ad23245436a057cff9d4aa3e8abccb5ffb222ec83e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: nh3, docopt, rfc3986, readme-renderer, pycodestyle, pkginfo, requests-toolbelt, autopep8, twine, autogen\n",
            "Successfully installed autogen-1.0.16 autopep8-2.0.4 docopt-0.6.2 nh3-0.2.15 pkginfo-1.9.6 pycodestyle-2.11.1 readme-renderer-42.0 requests-toolbelt-1.0.0 rfc3986-2.0.0 twine-4.0.2\n",
            "Collecting pyautogen~=0.2.0b4\n",
            "  Downloading pyautogen-0.2.6-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker\n",
            "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.2.0b4) (5.6.3)\n",
            "Collecting flaml (from pyautogen~=0.2.0b4)\n",
            "  Downloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=1.3 (from pyautogen~=0.2.0b4)\n",
            "  Using cached openai-1.7.2-py3-none-any.whl (212 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.2.0b4) (1.10.13)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.2.0b4) (1.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.2.0b4) (2.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.2.0b4) (0.5.2)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker) (23.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen~=0.2.0b4) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker) (2023.11.17)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.10/dist-packages (from flaml->pyautogen~=0.2.0b4) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen~=0.2.0b4) (2023.6.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen~=0.2.0b4) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen~=0.2.0b4) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen~=0.2.0b4) (0.14.0)\n",
            "Installing collected packages: flaml, docker, openai, pyautogen\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.1\n",
            "    Uninstalling openai-0.28.1:\n",
            "      Successfully uninstalled openai-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed docker-7.0.0 flaml-2.1.1 openai-1.7.2 pyautogen-0.2.6\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "!pip install -q langchain openai chromadb\n",
        "!pip install tiktoken\n",
        "!pip install openai==0.28.1\n",
        "!pip install autogen\n",
        "!pip install pyautogen~=0.2.0b4 docker\n",
        "!pip install fuzzywuzzy\n",
        "!pip install --upgrade typing_extensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "import praw\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "FTU_Vn1-nE9s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 's2'\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def get_completion_from_message(messages, model=\"gpt-3.5-turbo\", temperature=0 ):\n",
        "  response =client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        #temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "#     print(str(response.choices[0].message))\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "QQyy7kTvnM1b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_posts(kayword):\n",
        "  reddit = praw.Reddit(\n",
        "    client_id=\"ObBuaI6C5Uli56DiWQltKw\",\n",
        "    client_secret=\"gHt6ml6pg1MVYX-kzHjr45ltNz2UuA\",\n",
        "    redirect_uri=\"http://localhost:8080\",\n",
        "    user_agent=\"python:StockScraper:v1.0 (by /u/Asleep-Pause2513)\",\n",
        "    heck_for_async=False\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Specify the subreddit and initial limit\n",
        "  subreddit_name = kayword\n",
        "  limit = 1000  # The maximum limit per request\n",
        "\n",
        "  # Initialize an empty list to store submissions\n",
        "  all_submissions = []\n",
        "\n",
        "  # Initial request\n",
        "  submissions = reddit.subreddit(subreddit_name).new(limit=100000)\n",
        "\n",
        "  # Append the submissions to the list\n",
        "  all_submissions.extend(submissions)\n",
        "\n",
        "  # Fetch more data using the 'after' parameter\n",
        "  while True:\n",
        "      try:\n",
        "          # Get the last submission in the current batch\n",
        "          last_submission = submissions[-1]\n",
        "\n",
        "          # Make the next request using 'after' parameter to paginate\n",
        "          submissions = reddit.subreddit(subreddit_name).new(limit=limit, params={\"after\": last_submission.name})\n",
        "\n",
        "          # Check if there are more submissions, and append them\n",
        "          if not submissions:\n",
        "              break\n",
        "          all_submissions.extend(submissions)\n",
        "\n",
        "      except Exception as e:\n",
        "          print(\"An error occurred:\", str(e))\n",
        "          break\n",
        "\n",
        "  post_titles = []\n",
        "  post_urls = []\n",
        "  post_scores = []\n",
        "  post_comments_nums=[]\n",
        "  post_comments_texts = []\n",
        "  post_flair=[]\n",
        "  comments=[]\n",
        "  post_authors=[]\n",
        "  post_texts=[]\n",
        "  comments_score=[]\n",
        "  for i in all_submissions :\n",
        "    post_urls.append(i.url)\n",
        "    post_scores.append(i.score)\n",
        "    post_titles.append(i.title)\n",
        "    post_flair.append(i.link_flair_text)\n",
        "    post_comments_nums.append(i.num_comments)\n",
        "    post_authors.append(i.author.name if i.author else 'Deleted')\n",
        "\n",
        "    # Ensure that we don't get \"MoreComments\" objects which are placeholders\n",
        "    i.comments.replace_more(limit=0)\n",
        "\n",
        "    # Sort the comments by score in descending order and get the top 5\n",
        "    top_comments = sorted(i.comments.list(), key=lambda comment: comment.score, reverse=True)[:5]\n",
        "    # Now, extract the data for the top 5 comments\n",
        "    top_comments_texts = [comment.body for comment in top_comments]\n",
        "    top_comments_scores = [comment.score for comment in top_comments]\n",
        "\n",
        "    comments_score.append(top_comments_scores)\n",
        "    comments.append(top_comments_texts)\n",
        "\n",
        "    post_texts.append(i.selftext)  # Fetching the full text of the post\n",
        "  df = pd.DataFrame({\n",
        "    'Title': post_titles,\n",
        "    'URL': post_urls,\n",
        "    'Score': post_scores,\n",
        "    'Author': post_authors,\n",
        "    \"comments\":post_comments_nums,\n",
        "    'Text': post_texts,\n",
        "    \"post_flair\" : post_flair,\n",
        "    \"top comments\" : comments,\n",
        "    \"comments score\" :comments_score\n",
        "\n",
        "\n",
        "})\n",
        "  return df"
      ],
      "metadata": {
        "id": "NpzLdBgcncrR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=get_posts(\"investing\")\n",
        "df2=get_posts(\"Trading\")\n",
        "df4=get_posts(\"Cryptocurrencies\")\n",
        "df3=get_posts(\"stocks\")\n"
      ],
      "metadata": {
        "id": "5l2bgpT_ocpE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDRm2XGfuLjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly_xaQ-x1t0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['category'] =\"investing\"\n",
        "df2['category'] = \"trading\"\n",
        "df3['category'] = \"stocks\"\n",
        "df4['category'] = \"Cryptocurrencies\""
      ],
      "metadata": {
        "id": "OZyoqtEjxpRK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3 , df4], axis=0)\n",
        "combined_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSJpcb9xyKcc",
        "outputId": "6d212e1b-5326-4237-d29e-233b6c7a5b73"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3009, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df5=get_posts(\"mutualfunds\")\n",
        "df5[\"category\"] =\"mutual funds\""
      ],
      "metadata": {
        "id": "NM5A6IEr1AK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3 , df4 , df5], axis=0)\n",
        "combined_df.shape\n",
        "\n",
        "combined_df.to_csv('reddit_data', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfXsR4H27b_e",
        "outputId": "67191d80-b648-4c62-f99c-13a5848a47b6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3872, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.to_csv('reddit_data', index=False)"
      ],
      "metadata": {
        "id": "dYIbV-ho7qlY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6=get_posts(\"StockMarket\")\n",
        "df6[\"category\"] =\"StockMarket\""
      ],
      "metadata": {
        "id": "64Joxz3s4_fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6[\"category\"] =\"stocks\""
      ],
      "metadata": {
        "id": "zD7OUwT392dW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3 , df4 , df5], axis=0)\n",
        "\n",
        "\n",
        "combined_df.to_csv('reddit_data', index=False)"
      ],
      "metadata": {
        "id": "dYYuBPe18red"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epzHFdyHCIwN",
        "outputId": "ee427f83-a92e-4930-92e2-9ac79bd3433d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3872, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df7=get_posts(\"InvestmentClub\")\n",
        "df7[\"category\"] =\"investing\"\n",
        "\n"
      ],
      "metadata": {
        "id": "vbDDBDh18rtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3 , df4 , df5 ,df6], axis=0)\n",
        "\n",
        "\n",
        "combined_df.to_csv('reddit_data', index=False)"
      ],
      "metadata": {
        "id": "J4WMTCWZB4n1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hRjjpbQEWIm",
        "outputId": "bce29978-8bf1-4d6f-a5bd-5c168fb3c946"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4658, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df8=get_posts(\"StocksAndTrading\")\n",
        "df8[\"category\"] =\"StocksAndTrading\""
      ],
      "metadata": {
        "id": "JydW49SZ9UOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df8.shape\n",
        "combined_df = pd.concat([df1, df2, df3 , df4 , df5 ,df6 , df8,df7], axis=0)\n",
        "\n",
        "\n",
        "combined_df.to_csv('reddit_data.csv', index=False)"
      ],
      "metadata": {
        "id": "vkPJD7PJED9e"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KthF39MEk6h",
        "outputId": "56445f20-1945-443d-9d1b-1c183d6a8817"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5683, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3 , df4 , df5 ,df6 , df8 , df7 ], axis=0)\n",
        "\n",
        "\n",
        "combined_df.to_csv('reddit_data', index=False)"
      ],
      "metadata": {
        "id": "_Ey6nf9HFCjn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset= pd.read_csv(\"/content/reddit_data\")"
      ],
      "metadata": {
        "id": "4mNqzr80FGcl"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}
